# -*- coding: utf-8 -*-
"""Deployment

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rFiNGnt2FzgI4OB_JOEwTS-JCDXudcgi

# Importing the Neccessary Libraries
"""

import pandas as pd
import numpy as np
#import seaborn as sns
#import matplotlib.pyplot as plt
#import plotly.graph_objects as go
#from wordcloud import WordCloud
#from google.colab import drive
import os

"""# Importing the Data"""

#drive.mount('/content/drive')

train=pd.read_csv('/content/drive/MyDrive/Project/train.tsv',sep='\t')
test=pd.read_csv('/content/drive/MyDrive/Project/test.tsv',sep='\t')

"""Specifying the Train and Test sizes"""

train_size=len(train)
test_size=len(test)

train_data=train.copy()
test_data=test.copy()

train_data.head()

test_data.head()

"""Creating a set combining Train & Test data. Applying Count Vectorizer on combined set will help us to get the list of all possible words."""

combined_data = pd.concat([train_data,test_data])

combined_data.head()

# Submission set containing only the test IDs
submission = test_data[['test_id']]

combined_data.shape

# Taking a fraction (10%) of combined data set for experimentation. Dropping train/test ids here
combined_frac = combined_data.sample(frac=0.1).reset_index(drop=True)

combined_frac.shape

"""**Advanced Text Pre-Processing**

The steps we will apply for advanced text pre-processing are:

* Removing Puncuations
* Removing Digits
* Removing Stopwords
* Changing to Lower-case words
* Lemmatization or Stemming
"""

from string import punctuation
punctuation

# Create a list of punctuation replacements
punctuation_symbols = []
for symbol in punctuation:
    punctuation_symbols.append((symbol, ''))

# Remove Punctuation
import string
def remove_punctuation(sentence: str) -> str:
    return sentence.translate(str.maketrans('', '', string.punctuation))

# Remove Digits
def remove_digits(x):
    x = ''.join([i for i in x if not i.isdigit()])
    return x

# Remove Stopwords
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
STOPWORDS = set(stopwords.words('english'))

# Remove Stopwords
def remove_stop_words(x):
    x = ' '.join([i for i in x.lower().split(' ') if i not in STOPWORDS])
    return x

# Change to LowerCase Words
def to_lower(x):
    return x.lower()

# Segregating "category_name" into "category_main", "subcat_1", "subcat_2" like we did before 
def transform_category_name(category_name):
    try:
        main, sub1, sub2= category_name.split('/')
        return main, sub1, sub2
    except:
        return np.nan, np.nan, np.nan

train_data['category_main'], train_data['subcat_1'], train_data['subcat_2'] = zip(*train_data['category_name'].apply(transform_category_name))
cat_train = train_data[['category_main','subcat_1','subcat_2', 'price']]

train_data.head()

"""**Item Decription Analysis**"""

# Remove Digits, Punctuation, Stopwords, Converting to Lower-case and See the Effect
combined_data.item_description = combined_data.item_description.astype(str)
descr = combined_data[['item_description', 'price']]
descr['count'] = descr['item_description'].apply(lambda x : len(str(x)))
descr['item_description'] = descr['item_description'].apply(remove_digits)
descr['item_description'] = descr['item_description'].apply(remove_punctuation)
descr['item_description'] = descr['item_description'].apply(remove_stop_words)
descr.head(20)

from nltk.stem.porter import PorterStemmer
porter = PorterStemmer()
descr['item_description'] = descr['item_description'].apply(porter.stem)
descr.tail(20)

"""**Handling Missing Values in Combined Data**"""

# Basic data imputation of missing values
def handle_missing_values(df):
  df['category_name'].fillna(value='missing',inplace=True)
  df['brand_name'].fillna(value='None',inplace=True)
  df['item_description'].fillna(value='None',inplace=True)

# Converts to Categorical Features 
def to_categorical(df):
    df['brand_name'] = df['brand_name'].astype('category')
    df['category_name'] = df['category_name'].astype('category')
    df['item_condition_id'] = df['item_condition_id'].astype('category')

handle_missing_values(combined_frac)
to_categorical(combined_frac)

handle_missing_values(combined_data)
to_categorical(combined_data)

"""### Remove Digits, Punctuation, Stopwords, Converting to Lower-case for combined_frac"""

combined_frac.item_description = combined_frac.item_description.astype(str)
combined_frac['item_description'] = combined_frac['item_description'].apply(remove_digits)
combined_frac['item_description'] = combined_frac['item_description'].apply(remove_punctuation)
combined_frac['item_description'] = combined_frac['item_description'].apply(remove_stop_words)
combined_frac['item_description'] = combined_frac['item_description'].apply(to_lower)
combined_frac['name'] = combined_frac['name'].apply(remove_digits)
combined_frac['name'] = combined_frac['name'].apply(remove_punctuation)
combined_frac['name'] = combined_frac['name'].apply(remove_stop_words)
combined_frac['name'] = combined_frac['name'].apply(to_lower)
combined_frac.head()

"""### Remove Digits, Punctuation, Stopwords, Converting to Lower-case for combined_data"""

combined_data.item_description = combined_data.item_description.astype(str)
combined_data['item_description'] = combined_data['item_description'].apply(remove_digits)
combined_data['item_description'] = combined_data['item_description'].apply(remove_punctuation)
combined_data['item_description'] = combined_data['item_description'].apply(remove_stop_words)
combined_data['item_description'] = combined_data['item_description'].apply(to_lower)
combined_data['name'] = combined_data['name'].apply(remove_digits)
combined_data['name'] = combined_data['name'].apply(remove_punctuation)
combined_data['name'] = combined_data['name'].apply(remove_stop_words)
combined_data['name'] = combined_data['name'].apply(to_lower)
combined_data.head()

"""Applying CountVectorizer / TfidfVectorizer / LabelBinarizer
* CountVectorizer counts word frequencies.
* TF-IDF Vectorizer gives more significance (puts more weights) on rare words, and less significance (puts lesser weights) on frequent words.
* Label Binarizer converts labels into numeric representations for e.g. "A,B,C" -> [1,2,3]
"""

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.preprocessing import LabelBinarizer

# Apply Count Vectorizer to "name", this converts it into a sparse matrix 
cv = CountVectorizer(min_df=10)
X_name = cv.fit_transform(combined_data['name'])

# Apply Count Vectorizer to "category_name", this converts it into a sparse matrix
cv = CountVectorizer()
X_category = cv.fit_transform(combined_data['category_name'])

# Apply TFIDF to "item_description", 
tv = TfidfVectorizer(max_features=55000, ngram_range=(1, 2), stop_words='english')
X_description = tv.fit_transform(combined_data['item_description'])

# Apply LabelBinarizer to "brand_name"
lb = LabelBinarizer(sparse_output=True)
X_brand = lb.fit_transform(combined_data['brand_name'])

# vstack - adds rows
# hstack - adds columns
# csr_matrix - handles sparse matrix

from scipy.sparse import vstack, hstack, csr_matrix
X_dummies = csr_matrix(pd.get_dummies(combined_data[['item_condition_id', 'shipping']], sparse=True).values)

# Create the final sparse matrix combining everything together
sparse_merge = hstack((X_dummies, X_description, X_brand, X_category, X_name)).tocsr()

"""**Splitting Train and Test Data**"""

X_train_sparse = sparse_merge[:train_size]
X_test = sparse_merge[train_size:]

"""**Creating validation data using train data**"""

from sklearn.model_selection import KFold
kf = KFold(n_splits=3, shuffle=True, random_state=12345)
y = np.log1p(train_data['price'])
i = 0;
for train_indicies, valid_indicies in kf.split(X_train_sparse):
    X_train, y_train = X_train_sparse[train_indicies], y[train_indicies]
    X_valid, y_valid = X_train_sparse[valid_indicies], y[valid_indicies]

"""**Funtion to Run the Model**"""

from sklearn.metrics import mean_squared_error, r2_score
def run_model_advText(model, X_train, y_train, X_valid, y_valid, verbose = False):
    model.fit(X_train, y_train)
    preds_valid = model.predict(X_valid)
    mse = mean_squared_error(y_valid,preds_valid)
    r_sq = r2_score(y_valid,preds_valid)
    print("Mean Squared Error Value : "+"{:.2f}".format(mse))
    print("R-Squared Value : "+"{:.2f}".format(r_sq))
    return model, mse, r_sq

from sklearn import linear_model
import lightgbm
import xgboost


"""**Storing in pickle file**"""

regression_model=linear_model.Ridge(solver = "saga", fit_intercept=False)
regression_model.fit(X_train, y_train)

import pickle 
pickle_out = open("Regression_Model.pkl", mode = "wb") 
pickle.dump(regression_model, pickle_out) 
pickle_out.close()


import streamlit as st
 
# loading the trained model
pickle_in = open('Regression_Model.pkl', 'rb') 
Regression_Model = pickle.load(pickle_in)
 
@st.cache()

# defining the function which will make the prediction using the data which the user inputs 
def prediction(name, item_condition_id, category_name, brand_name, shipping, item_description):   
 
    # Making predictions 
    prediction = Regression_Model.predict( 
        [[name, item_condition_id, category_name, brand_name, shipping, item_description]])
        
        
# this is the main function in which we define our webpage  
def main():       
   # front end elements of the web page 
   html_temp = """ 
   <div style ="background-color:blue;padding:13px"> 
   <h1 style ="color:black;text-align:center;">Mercari Price Suggestion ML App</h1> 
   </div> 
     """
      
   # display the front end aspect
   st.markdown(html_temp, unsafe_allow_html = True) 
      
   # following lines create boxes in which user can enter data required to make prediction 
   name = st.text('name')
   item_condition_id = st.selectbox('item_condition_id',(1,2,3,4,5))
   category_name = st.text('category_name')
   brand_name = st.text('brand_name')
   shipping = st.selectbox('shipping',(1,0))
   item_description = st.text('item_description')
   result =""
      
   # when 'Predict' is clicked, make the prediction and store it 
   if st.button("Predict"): 
       result = prediction(name, item_condition_id, category_name, brand_name, shipping, item_description) 
       st.success('Price for the product is {}'.format(result))
        
     
if __name__=='__main__': 
    main()
  

