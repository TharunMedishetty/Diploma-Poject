# -*- coding: utf-8 -*-
"""Proj_29_phase_05__TharunMedishetty

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rFiNGnt2FzgI4OB_JOEwTS-JCDXudcgi

# Importing the Neccessary Libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.graph_objects as go
from wordcloud import WordCloud
from google.colab import drive
import os

"""# Importing the Data"""

drive.mount('/content/drive')

train=pd.read_csv('/content/drive/MyDrive/Project/train.tsv',sep='\t')
test=pd.read_csv('/content/drive/MyDrive/Project/test.tsv',sep='\t')

"""Specifying the Train and Test sizes"""

train_size=len(train)
test_size=len(test)

train_data=train.copy()
test_data=test.copy()

train_data.head()

test_data.head()

"""Creating a set combining Train & Test data. Applying Count Vectorizer on combined set will help us to get the list of all possible words."""

combined_data = pd.concat([train_data,test_data])

combined_data.head()

# Submission set containing only the test IDs
submission = test_data[['test_id']]

combined_data.shape

# Taking a fraction (10%) of combined data set for experimentation. Dropping train/test ids here
combined_frac = combined_data.sample(frac=0.1).reset_index(drop=True)

combined_frac.shape

"""**Advanced Text Pre-Processing**

The steps we will apply for advanced text pre-processing are:

* Removing Puncuations
* Removing Digits
* Removing Stopwords
* Changing to Lower-case words
* Lemmatization or Stemming
"""

from string import punctuation
punctuation

# Create a list of punctuation replacements
punctuation_symbols = []
for symbol in punctuation:
    punctuation_symbols.append((symbol, ''))

# Remove Punctuation
import string
def remove_punctuation(sentence: str) -> str:
    return sentence.translate(str.maketrans('', '', string.punctuation))

# Remove Digits
def remove_digits(x):
    x = ''.join([i for i in x if not i.isdigit()])
    return x

# Remove Stopwords
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
STOPWORDS = set(stopwords.words('english'))

# Remove Stopwords
def remove_stop_words(x):
    x = ' '.join([i for i in x.lower().split(' ') if i not in STOPWORDS])
    return x

# Change to LowerCase Words
def to_lower(x):
    return x.lower()

# Segregating "category_name" into "category_main", "subcat_1", "subcat_2" like we did before 
def transform_category_name(category_name):
    try:
        main, sub1, sub2= category_name.split('/')
        return main, sub1, sub2
    except:
        return np.nan, np.nan, np.nan

train_data['category_main'], train_data['subcat_1'], train_data['subcat_2'] = zip(*train_data['category_name'].apply(transform_category_name))
cat_train = train_data[['category_main','subcat_1','subcat_2', 'price']]

train_data.head()

"""**Item Decription Analysis**"""

# Remove Digits, Punctuation, Stopwords, Converting to Lower-case and See the Effect
combined_data.item_description = combined_data.item_description.astype(str)
descr = combined_data[['item_description', 'price']]
descr['count'] = descr['item_description'].apply(lambda x : len(str(x)))
descr['item_description'] = descr['item_description'].apply(remove_digits)
descr['item_description'] = descr['item_description'].apply(remove_punctuation)
descr['item_description'] = descr['item_description'].apply(remove_stop_words)
descr.head(20)

from nltk.stem.porter import PorterStemmer
porter = PorterStemmer()
descr['item_description'] = descr['item_description'].apply(porter.stem)
descr.tail(20)

"""**Handling Missing Values in Combined Data**"""

# Basic data imputation of missing values
def handle_missing_values(df):
  df['category_name'].fillna(value='missing',inplace=True)
  df['brand_name'].fillna(value='None',inplace=True)
  df['item_description'].fillna(value='None',inplace=True)

# Converts to Categorical Features 
def to_categorical(df):
    df['brand_name'] = df['brand_name'].astype('category')
    df['category_name'] = df['category_name'].astype('category')
    df['item_condition_id'] = df['item_condition_id'].astype('category')

handle_missing_values(combined_frac)
to_categorical(combined_frac)

handle_missing_values(combined_data)
to_categorical(combined_data)

"""### Remove Digits, Punctuation, Stopwords, Converting to Lower-case for combined_frac"""

combined_frac.item_description = combined_frac.item_description.astype(str)
combined_frac['item_description'] = combined_frac['item_description'].apply(remove_digits)
combined_frac['item_description'] = combined_frac['item_description'].apply(remove_punctuation)
combined_frac['item_description'] = combined_frac['item_description'].apply(remove_stop_words)
combined_frac['item_description'] = combined_frac['item_description'].apply(to_lower)
combined_frac['name'] = combined_frac['name'].apply(remove_digits)
combined_frac['name'] = combined_frac['name'].apply(remove_punctuation)
combined_frac['name'] = combined_frac['name'].apply(remove_stop_words)
combined_frac['name'] = combined_frac['name'].apply(to_lower)
combined_frac.head()

"""### Remove Digits, Punctuation, Stopwords, Converting to Lower-case for combined_data"""

combined_data.item_description = combined_data.item_description.astype(str)
combined_data['item_description'] = combined_data['item_description'].apply(remove_digits)
combined_data['item_description'] = combined_data['item_description'].apply(remove_punctuation)
combined_data['item_description'] = combined_data['item_description'].apply(remove_stop_words)
combined_data['item_description'] = combined_data['item_description'].apply(to_lower)
combined_data['name'] = combined_data['name'].apply(remove_digits)
combined_data['name'] = combined_data['name'].apply(remove_punctuation)
combined_data['name'] = combined_data['name'].apply(remove_stop_words)
combined_data['name'] = combined_data['name'].apply(to_lower)
combined_data.head()

"""Applying CountVectorizer / TfidfVectorizer / LabelBinarizer
* CountVectorizer counts word frequencies.
* TF-IDF Vectorizer gives more significance (puts more weights) on rare words, and less significance (puts lesser weights) on frequent words.
* Label Binarizer converts labels into numeric representations for e.g. "A,B,C" -> [1,2,3]
"""

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.preprocessing import LabelBinarizer

# Apply Count Vectorizer to "name", this converts it into a sparse matrix 
cv = CountVectorizer(min_df=10)
X_name = cv.fit_transform(combined_data['name'])

# Apply Count Vectorizer to "category_name", this converts it into a sparse matrix
cv = CountVectorizer()
X_category = cv.fit_transform(combined_data['category_name'])

# Apply TFIDF to "item_description", 
tv = TfidfVectorizer(max_features=55000, ngram_range=(1, 2), stop_words='english')
X_description = tv.fit_transform(combined_data['item_description'])

# Apply LabelBinarizer to "brand_name"
lb = LabelBinarizer(sparse_output=True)
X_brand = lb.fit_transform(combined_data['brand_name'])

# vstack - adds rows
# hstack - adds columns
# csr_matrix - handles sparse matrix

from scipy.sparse import vstack, hstack, csr_matrix
X_dummies = csr_matrix(pd.get_dummies(combined_data[['item_condition_id', 'shipping']], sparse=True).values)

# Create the final sparse matrix combining everything together
sparse_merge = hstack((X_dummies, X_description, X_brand, X_category, X_name)).tocsr()

"""**Splitting Train and Test Data**"""

X_train_sparse = sparse_merge[:train_size]
X_test = sparse_merge[train_size:]

"""**Creating validation data using train data**"""

from sklearn.model_selection import KFold
kf = KFold(n_splits=3, shuffle=True, random_state=12345)
y = np.log1p(train_data['price'])
i = 0;
for train_indicies, valid_indicies in kf.split(X_train_sparse):
    X_train, y_train = X_train_sparse[train_indicies], y[train_indicies]
    X_valid, y_valid = X_train_sparse[valid_indicies], y[valid_indicies]

"""**Funtion to Run the Model**"""

from sklearn.metrics import mean_squared_error, r2_score
def run_model_advText(model, X_train, y_train, X_valid, y_valid, verbose = False):
    model.fit(X_train, y_train)
    preds_valid = model.predict(X_valid)
    mse = mean_squared_error(y_valid,preds_valid)
    r_sq = r2_score(y_valid,preds_valid)
    print("Mean Squared Error Value : "+"{:.2f}".format(mse))
    print("R-Squared Value : "+"{:.2f}".format(r_sq))
    return model, mse, r_sq

from sklearn import linear_model
import lightgbm
import xgboost

"""**Storing in pickle file**"""

regression_model=linear_model.Ridge(solver = "saga", fit_intercept=False)
regression_model.fit(X_train, y_train)

import pickle 
pickle_out = open("Regression_Model.pkl", mode = "wb") 
pickle.dump(regression_model, pickle_out) 
pickle_out.close()

"""**Model-1: Ridge Regression**"""

ridge_reg = linear_model.Ridge(solver = "saga", fit_intercept=False)
print("Ridge Regression (After advanced Text Pre-processing)")
print("-----------------------------------------------------")
model_11, mse_11, r_sq_11 = run_model_advText(ridge_reg, X_train, y_train, X_valid, y_valid)

import math

print("RMSE:",math.sqrt(mse_11))

"""**Model-2: LGBM Regression**"""

lgbm_reg = lightgbm.LGBMRegressor()
print("LGBM Regression (After advanced Text Pre-processing)")
print("----------------------------------------------------")
model_22, mse_22, r_sq_22 = run_model_advText(lgbm_reg, X_train, y_train, X_valid, y_valid)

print("RMSE:",math.sqrt(mse_22))

"""**Model-3: XGB Regression** """

xgb_params = {'n_estimators':500, 'max_depth':8}
xgb_reg = xgboost.XGBRegressor(**xgb_params)
print("XGB Regression (After advanced Text Pre-processing)")
print("---------------------------------------------------")
model_33, mse_33, r_sq_33 = run_model_advText(xgb_reg, X_train, y_train, X_valid, y_valid)

print("RMSE:",math.sqrt(mse_33))

"""**Results without Advanced Text Pre-Processing**"""

data={'Model': ['Ridge', 'LGBM', 'XGB'],
        'MSE': [1464.71, 1035.26, 893.99],
      'R-Suared':[0.02,0.31,0.40]}
results_without_Adv_txt=pd.DataFrame(data)

results_without_Adv_txt

data={'Model': ['Ridge', 'LGBM', 'XGB'],
        'MSE': [0.22, 0.29, 0.25],
      'R-Suared':[0.60,0.48,0.55]}
results_with_Adv_txt=pd.DataFrame(data)

results_with_Adv_txt

"""Improvement in Model Performance after using Advanced Text Pre-Processing :
* MSE has now decreased for all models
* ùëÖ2  value has now increased for all models
However, for Ridge model, the improvement seems the highest. Hence, we will choose Ridge to apply on the test dataset.
"""

mse_before = [1464.71, 1035.26, 893.99]
r_sq_before = [0.02,0.31,0.40]
mse_after = [mse_11, mse_22, mse_33]
r_sq_after = [r_sq_11, r_sq_22, r_sq_33]
model_data = {'Model': ['Ridge','LGBM','XGB'],
              'MSE_without_Advanced_TextProcessing': mse_before,
              'R_Square_without_Advanced_TextProcessing': r_sq_before,
              'MSE_with_Advanced_TextProcessing': mse_after,
              'R_Square_with_Advanced_TextProcessing': r_sq_after}
data_compare = pd.DataFrame(model_data)

data_compare

"""**Apply Final Ridge Model on Test Dataset and Submission :**"""

predictions = ridge_reg.predict(X_test)
submission["price"] = np.expm1(predictions)
submission.head()

submission.to_csv("submission.csv", index = False)

submission

"""**Merging Price Column to the Test Data**"""

test_data_price=test_data.merge(submission, left_on='test_id', right_on='test_id',
          suffixes=('_left', '_right'))

test_data_price

